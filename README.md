# ai-critical-thinking

Recently SOTA language models such as GPT, BERT, and LLAMA have shown an impressive amount of general intelligence. GPT4 has even been shown to have an IQ of 130 and pass the bar. However, the ways these models work is by being trained on <i> existing </i> data that was created by humans. It still remains to be seen if language models can solve problems that are outside the limits of current human knowledge, which is necessary for the creation of superhuman intelligence.

In this repo, we explore this topic by creating small language models (SLM) that are trained on carefully curated datasets, and then trying to get them to solve problems that are just outside of it's current reach. In order to best test this, we believe it is best to train a SLM on basic algebra, and then giving it problems that are just a but more complicated than what it was trained on. This way, there is a clear objective measure to determine accuracy of it's predictions.

We have found that just simply asking the model to solve a problem outside of it's training data predictably leads to poor results. However, by having the model generate a response with gumbel noise and provide reinforcement learning with solver feedback (RLSF), and trying n number of times, it is able to learn things that are just outside of it's training set.

## Gathering training data
The initial training set was generated by asking GPT4 to provide some examples of basic algebra using only generic coefficients (no numbers). A snippet of data/train-very-basic.csv is shown below. Note the small size of this data set (only 541 examples).
```
Expand B: B(x - y) = A --> Bx - By = A
Add A to both sides: x + A = B --> x = B - A
Expand A: A(z - y) = B --> Az - Ay = B
Factor out A: Az + Ay = B --> A(z + y) = B
Combine like terms: z + z = x --> 2z = x
Move y: y^2 + D^3 = Ey^4 --> y^2 + D^3 = Ey^4
```

## Initial training
The base model was trained using a generic decoder with the adam optimizer and CrossEntropyLoss. Only 100 epochs were needed to acheive a loss of 0.35. The training can be ran with:
```
python3 src/basic_training.py
```

## Results

## No RLSF 
(src/test_model_without_reinforcement.py)

#### Ex 1 (inside training set): 
```
prompt = "Expand B: B(x - y) = A -->"
```
```
Result = "Expand B: B(x - y) = A --> Bx - By = A"
```

<div align="center"><b>Correct result.</b></div>

#### Ex 2 (just outside training set): 
```
prompt = "Expand B: B(x - y - z) = A -->"
```
```
Result = "Expand B: B(x - y - z) = A --> Bx - By = A"
```
<div align="center"><b>Incorrect result.</b></div>

## With RLSF 
(src/test_model.py)

#### Ex 1 (inside training set): 
```
prompt = "Expand B: B(x - y) = A -->"
```
```
Result = "Expand B: B(x - y) = A --> Bx - By = A"
```
<div align="center"><b>Correct result.</b></div>

#### Ex 2 (just outside training set): 
```
prompt = "Expand B: B(x - y - z) = A -->"
```
```
Result = "Expand B: B(x - y - z) = A --> Bx - Bz - By = A"
```
<div align="center"><b>Correct result!</b></div>
